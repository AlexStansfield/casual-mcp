# ğŸ§  Casual MCP

**Casual MCP** is a Python framework for building, evaluating, and serving LLMs with tool-calling capabilities using [Model Context Protocol (MCP)](https://modelcontextprotocol.io).  
It includes:

- âœ… A multi-server MCP client
- âœ… Provider support for OpenAI + Ollama
- âœ… A recursive tool-calling chat loop
- âœ… System prompt templating with Jinja2
- âœ… A FastAPI server with CLI support

## ğŸ”§ Installation

```bash
pip install casual-mcp
```

Or for development:

```bash
git clone https://github.com/AlexStansfield/casual-mcp.git
cd casual-mcp
uv pip install -e .[dev]
```

## ğŸ§© Providers

Providers allow access to LLMs, currently only an OpenAI provider is supplied. However in the model configuration you can supply an optional endpoint allowing you to use any `openai` compatible API (e.g LM Studio).

On the todo list is an Ollama provider and the ability to create your own custom providers to use.

---

## ğŸ§© System Prompt Templates

System prompts are defined as [Jinja2](https://jinja.palletsprojects.com) templates in the `prompt-templates/` directory.

They are used in the config file to specify a specific system prompt to use on the model.

This allows you to define custom system prompts for each model which is useful when using models that do not natively support tools. The templates are passed the tools list in the `tools` variable, so that you can supply the list of tools to the LLM in the prompt.

For example:
```
Here is a list of functions in JSON format that you can invoke:
[
{% for tool in tools %}
  {
    "name": "{{ tool.name }}",
    "description": "{{ tool.description }}",
    "parameters": {
    {% for param_name, param in tool.inputSchema.items() %}
      "{{ param_name }}": {
        "description": "{{ param.description }}",
        "type": "{{ param.type }}"{% if param.default is defined %},
        "default": "{{ param.default }}"{% endif %}
      }{% if not loop.last %},{% endif %}
    {% endfor %}
    }
  }{% if not loop.last %},{% endif %}
{% endfor %}
]
```

## âš™ï¸ Configuration File (`config.json`)

The CLI and API can be configured using a `config.json` file that defines:

- ğŸ”§ Available **models** and their providers
- ğŸ§° Available **MCP tool servers**
- ğŸ§© Optional tool namespacing behavior

### ğŸ”¸ Example

```json
{
  "namespaced_tools": false,
  "models": {
    "lm-qwen-3": {
      "provider": "openai",
      "endpoint": "http://localhost:1234/v1",
      "model": "qwen3-8b",
      "template": "lm-studio-native-tools"
    },
    "gpt-4.1": {
        "provider": "openai",
        "model": "gpt-4.1"
    }
  },
  "servers": {
    "time": {
      "type": "python",
      "path": "mcp-servers/time/server.py"
    },
    "weather": {
      "type": "http",
      "url": "http://localhost:5050/mcp"
    }
  }
}
```

### ğŸ”¹ `models`

Each model has:

- `provider`: `"openai"` or `"ollama"`
- `model`: the model name (e.g., `gpt-4.1`, `qwen3-8b`)
- `endpoint`: required for custom OpenAI-compatible backends (e.g., LM Studio)
- `template`: optional name used to apply model-specific tool calling formatting

### ğŸ”¹ `servers`

Each server has:

- `type`: `"python"`, `"http"`, `"node"`, or `"uvx"`
- For `python`/`node`: `path` to the script
- For `http`: `url` to the remote MCP endpoint
- For `uvx`: `package` for the package to run
- Optional: `env` for subprocess environments, `system_prompt` to override server prompt

### ğŸ”¹ `namespaced_tools`

If `true`, tools will be prefixed by server name (e.g., `weather-get_weather`).  
Useful for disambiguating tool names across servers and avoiding name collision if multiple servers have the same tool name.

## ğŸ›  CLI Reference

### `casual-mcp serve`
Start the API server.

**Options:**
- `--host`: Host to bind (default `0.0.0.0`)
- `--port`: Port to serve on (default `8000`)

### `casual-mcp servers`
Loads the config and outputs the list of MCP servers you have configured.

#### Example Output
```
$ casual-mcp servers
â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”“
â”ƒ Name    â”ƒ Type   â”ƒ Path / Package / Url          â”ƒ Env â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”©
â”‚ math    â”‚ python â”‚ mcp-servers/math/server.py    â”‚     â”‚
â”‚ time    â”‚ python â”‚ mcp-servers/time-v2/server.py â”‚     â”‚
â”‚ weather â”‚ python â”‚ mcp-servers/weather/server.py â”‚     â”‚
â”‚ words   â”‚ python â”‚ mcp-servers/words/server.py   â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

### `casual-mcp models`
Loads the config and outputs the list of models you have configured.

#### Example Output
```
$ casual-mcp models
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Name              â”ƒ Provider â”ƒ Model                     â”ƒ Endpoint               â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ lm-phi-4-mini     â”‚ openai   â”‚ phi-4-mini-instruct       â”‚ http://kovacs:1234/v1  â”‚
â”‚ lm-hermes-3       â”‚ openai   â”‚ hermes-3-llama-3.2-3b     â”‚ http://kovacs:1234/v1  â”‚
â”‚ lm-groq           â”‚ openai   â”‚ llama-3-groq-8b-tool-use  â”‚ http://kovacs:1234/v1  â”‚
â”‚ gpt-4o-mini       â”‚ openai   â”‚ gpt-4o-mini               â”‚                        â”‚
â”‚ gpt-4.1-nano      â”‚ openai   â”‚ gpt-4.1-nano              â”‚                        â”‚
â”‚ gpt-4.1-mini      â”‚ openai   â”‚ gpt-4.1-mini              â”‚                        â”‚
â”‚ gpt-4.1           â”‚ openai   â”‚ gpt-4.1                   â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ API Usage

### Start the API Server

```bash
casual-mcp serve --host 0.0.0.0 --port 8000
```

You can then POST to `/chat` to trigger tool-calling LLM responses.

The request takes a json body consisting of:
- `model`: the LLM model to use
- `user_prompt`: the user prompt string
- `messages`: list of chat messages (system, assistant, user, etc) that you can pass to the api, allowing you to keep your own chat session in the client calling the api
- `session_id`: an optional ID that stores all the messages from the session and provides them back to the LLM for context

You can either pass in a `user_prompt` or a list of `messages` depending on your use case.

Example:
```
{
    "session_id": "my-test-session",
    "model": "gpt-4o-mini",
    "user_prompt": "can you explain what the word consistent means?"
}
```

## License

This software is released under the [MIT License](LICENSE)